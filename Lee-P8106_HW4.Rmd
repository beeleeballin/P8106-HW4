---
title: "P8106 HW4"
author: "Brian Jo Hsuan Lee"
date: "3/31/2022"
output: pdf_document
---

Load packages
```{r, message=F}
library(tidyverse)
library(caret)
library(rpart.plot)
library(ranger)
library(gbm)
library(knitr)
library(party)
library(ISLR)
library(pROC)
```

\newpage

## Problem 1: How Much is Your Out-of-State Tuition?

Load and split data into training and testing sets
```{r, message=F}
set.seed(2022)

# import and tidy
data = read_csv("./College.csv") %>% 
  janitor::clean_names() %>% 
  select(-college)

# partition data into training and testing sets as randomized 4:1 splits
train_index = createDataPartition(y = data$outstate, p = 0.8, list = F)
train_data = data[train_index, ]
test_data = data[-train_index, ]

# testing set response for RMSE calculation
test_resp = test_data$outstate
```

Set cross validation methods
```{r}
# for regression tree
ctrl_re = trainControl(method = "repeatedcv", number = 2, repeats = 5)

# for classification tree under the minimal MSE rule
ctrl = trainControl(method = "repeatedcv", number = 2, repeats = 5,
                       summaryFunction = twoClassSummary,
                       classProbs = TRUE)

# for classification tree under the 1SE rule
ctrl_1se = trainControl(method = "repeatedcv", number = 2, repeats = 5, 
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        selectionFunction = "oneSE")
```

\newpage

a) 
**Fit and plot a regression tree model**

Use the regression tree (CART) approach to graph an optimally pruned tree. At the top (root) of the tree, it is shown that splitting at `expend` over or under 11K provides significantly more accurate predictions for out-of-state tuitions than any other.
```{r}
set.seed(2022)

rpart_grid = data.frame(cp = exp(seq(-8,-5, length = 100)))
rpart_fit = train(outstate ~ . , 
                  data,
                  subset = train_index,
                  method = "rpart",
                  tuneGrid = rpart_grid,
                  trControl = ctrl_re)
# ggplot(rpart_fit, highlight = TRUE)

rpart.plot(rpart_fit$finalModel)
```

For comparison, the following is the code using the conditional inference tree (CIT) approach. The code generates an overly cluttered graph but `expend` is still atop the decision tree. 
```{r, eval=F}
set.seed(2022)

ctree_grid = data.frame(mincriterion = 1-exp(seq(-2, 0, length = 100)))
ctree_fit = train(outstate ~ . , 
                  data, 
                  subset = train_index,
                  method = "ctree",
                  tuneGrid = ctree_grid,
                  trControl = ctrl_re)
ggplot(ctree_fit, highlight = TRUE)

plot(ctree_fit$finalModel)

RMSE(predict(ctree_fit, newdata = test_data), test_resp)
```

\newpage

b) 
**Fit and evaluate a random forest regression model**

```{r}
set.seed(2022)

rf_grid = expand.grid(mtry = 1:16,
                      splitrule = "variance",
                      min.node.size = 1:6)
rf_fit = train(outstate ~ . , 
               data,
               subset = train_index,
               method = "ranger",
               tuneGrid = rf_grid,
               trControl = ctrl_re)
# ggplot(rf_fit, highlight = TRUE)
```

Calculate and graph variable importance using permutation and impurity metrics. Similarly, both evaluations suggest `expend` as the most important predictor for regressing out-of-state tuition, followed by `room-board`. 
```{r}
set.seed(2022)

rf_perm = ranger(outstate ~ . , 
                 train_data,
                 mtry = rf_fit$bestTune[[1]], 
                 splitrule = "variance",
                 min.node.size = rf_fit$bestTune[[3]],
                 importance = "permutation",
                 scale.permutation.importance = TRUE)
barplot(sort(importance(rf_perm), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))

rf_imp = ranger(outstate ~ . , 
                train_data,
                mtry = rf_fit$bestTune[[1]], 
                splitrule = "variance",
                min.node.size = rf_fit$bestTune[[3]],
                importance = "impurity") 
barplot(sort(importance(rf_imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))
```

For the random forest model test error and its interpretation, see the end of part C).

\newpage

c) 
**Fit and evaluate a gradient boosting regression model**

```{r}
set.seed(2022)

gbm_grid = expand.grid(n.trees = c(2000, 3000, 4000, 5000),
                       interaction.depth = 1:5,
                       shrinkage = c(0.001,0.003,0.005),
                       n.minobsinnode = c(1, 10))
gbm_fit = train(outstate ~ . , 
                train_data, 
                method = "gbm",
                tuneGrid = gbm_grid,
                trControl = ctrl_re,
                verbose = FALSE)
# ggplot(gbm_fit, highlight = TRUE)
```

Calculate, list and graph variable importance. Again, boosting suggests `expend` and `room-board` as the 2 most important predictors for regressing out-of-state tuition. 
```{r}
summary(gbm_fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

Show test errors for both the random forest and boosting models, and compare them with their cross-validation errors. The boosting model has a lower test error and cross-validation error than those of the random forest model. Notice both their test RMSEs fall in the 4th quartile of their cross-validation errors, which is rather high but still within expectation, and both models could be applied to other new testing sets. 
```{r}
rf_test_rmse = RMSE(predict(rf_fit, newdata = test_data), test_resp)
boost_test_rmse = RMSE(predict(gbm_fit, newdata = test_data), test_resp)
kable(c(rf = rf_test_rmse, boost = boost_test_rmse), col.names = "RMSE", "simple")
summary(resamples(list(rf = rf_fit, boost = gbm_fit)))
```

```{r, eval=F, echo=F}
rf_part = partial(rf_fit, pred.var = "expend", 
                  plot = TRUE, rug = TRUE, 
                  plot.engine = "ggplot") + ggtitle("PDP (RF)")
gbm_part = partial(gbm_fit, pred.var = "expend", 
                   plot = TRUE, rug = TRUE, 
                   plot.engine = "ggplot") + ggtitle("PDP (GBM)")
grid.arrange(rf_part, gbm_part, nrow = 1)
```

\newpage

## Problem 2: Citrus Hill or Minute Maid?

Load and split data into training and testing sets.
```{r}
set.seed(2022)

data2 =
  OJ %>% 
  janitor::clean_names() %>% 
  mutate(
    purchase = factor(purchase),
    store_id = factor(store_id),
    store = factor(store)
  ) %>% 
  drop_na()

# partition data into training and testing sets into randomized 4:1 splits
train_index2 = createDataPartition(y = data2$purchase, p = (699/1070), list = F)
train_data2 = data2[train_index2, ]
test_data2 = data2[-train_index2, ]

# testing set response for RMSE calculation
test_resp2 = test_data2$purchase
```

\newpage

a) 
**Find a classification method with the lower validation error**

CART and CIT approaches under the minimal MSE Rule (code for CIT not evaluated)
```{r}
set.seed(2022)

rpart_grid2 = data.frame(cp = exp(seq(-16,-4, length = 100)))

rpart_fit2 = train(purchase ~ . , 
                   data2,
                   subset = train_index2, 
                   method = "rpart",
                   tuneGrid = rpart_grid2,
                   trControl = ctrl,
                   metric = "ROC")
# ggplot(rpart_fit2, highlight = TRUE)

rpart.plot(rpart_fit2$finalModel)
```

```{r, eval=F}
set.seed(2022)

ctree_grid2 = data.frame(mincriterion = 1-exp(seq(-3, 0, length = 100)))

ctree_fit2 = train(purchase ~ . , 
                   data2, 
                   subset = train_index2,
                   method = "ctree",
                   tuneGrid = ctree_grid2,
                   trControl = ctrl)
ggplot(ctree_fit2, highlight = TRUE)

plot(ctree_fit2$finalModel)
```

CART and CIT approaches under the 1SE rule (code for CIT not evaluated)
```{r, warning=F}
set.seed(2022)
rpart_fit2_1se = train(purchase ~ . , 
                 data2,
                 subset = train_index2, 
                 method = "rpart",
                 tuneGrid = rpart_grid2,
                 trControl = ctrl_1se)
# ggplot(rpart_fit2_1se, highlight = TRUE)

rpart.plot(rpart_fit2_1se$finalModel)
```

```{r, eval=F}
set.seed(2022)

ctree_fit2_1se = train(purchase ~ . , 
                       data2, 
                       subset = train_index2,
                       method = "ctree",
                       tuneGrid = ctree_grid2,
                       trControl = ctrl_1se)
ggplot(ctree_fit2_1se, highlight = TRUE)

plot(ctree_fit2_1se$finalModel)
```

The 2 trees are differently sized, though they share similar splits. The optimized classification tree under the 1SE rule splits at all the thresholds at which the minimal MSE tree splits, but also 13 additional splits, creating a total of 18 terminal nodes instead of the 5 from the minimal MSE tree. As a result, there is a slight difference in cross-validation errors. In terms of ROC, even as they share a similarly sized IQR, the 1SE model has a lower spanning IQR and a higher mean.
```{r}
train_res = resamples(list(MSE = rpart_fit2, `1SE` = rpart_fit2_1se))
bwplot(train_res, metric = "ROC")
```

```{r, eval=F, echo=F}
rpart_pred = predict(rpart_fit2, newdata = test_data2, type = "prob")[,1]
ctree_pred = predict(ctree_fit2, newdata = test_data2, type = "prob")[,1]
rpart_1se_pred = predict(rpart_fit2_1se, newdata = test_data2, type = "prob")[,1]
ctree_1se_pred = predict(ctree_fit2_1se, newdata = test_data2, type = "prob")[,1]

modelNames = c("rpart", "ctree", "rpart 1se", "ctree 1se")

rpart_roc = roc(test_resp2, rpart_pred)
ctree_roc = roc(test_resp2, ctree_pred)
rpart_1se_roc = roc(test_resp2, rpart_1se_pred)
ctree_1se_roc = roc(test_resp2, ctree_1se_pred)

auc = c(rpart_roc$auc[1], 
        ctree_roc$auc[1], 
        rpart_1se_roc$auc[1], 
        ctree_1se_roc$auc[1])

plot(rpart_roc, legacy.axes = TRUE)
lines(ctree_roc, col = 2, lty = 2)
lines(rpart_1se_roc, col = 3, lty = 3)
lines(ctree_1se_roc, col = 4, lty = 4)
legend("bottomright", 
       legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:4, lwd = 2)

# the CIT trees have lower test errors than the classification tree, under both the minimal MSE and 1SE rules.
```

\newpage

b)
**Test error of a Adaboost classification tree**

Fit and graph predictor hierarchy by importance. `loyal_ch` and `price_diff` have a high relative influence on the classification compared to all other predictors, and the model performs at 0.926 AUC. 
```{r, message=F}
set.seed(2022)

gbm_grid2 = expand.grid(n.trees = c(2000, 3000, 4000, 5000),
                        interaction.depth = 1:6,
                        shrinkage = c(0.0005,0.001,0.002),
                        n.minobsinnode = c(1, 10))
gbm_fit2 = train(purchase ~ . , 
                 train_data2, 
                 method = "gbm",
                 tuneGrid = gbm_grid2,
                 trControl = ctrl,
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
# ggplot(gbm_fit2, highlight = TRUE)

summary(gbm_fit2$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

```{r}
gbm_pred = predict(gbm_fit2, newdata = test_data2, type = "prob")[,1]
gbm_roc = roc(test_resp2, gbm_pred)

plot(gbm_roc, col = 1)
legend("bottomright", 
       legend = paste0("Adaboost: ", round(gbm_roc$auc[1], 3)),
       col = 1, lwd = 2)
```

